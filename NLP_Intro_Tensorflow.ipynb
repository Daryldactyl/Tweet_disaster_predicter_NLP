{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d50d9b-4977-4cdc-a52c-897982c2ad2c",
   "metadata": {},
   "source": [
    "# Introduction to NLP Fundamentals in TensorFlow\n",
    "\n",
    "NLP has the goal of deriving information out of natural language (could be sequences of text or speech).\n",
    "\n",
    "Another common term for NLP problems is sequence to sequence problems (seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bac9e15-f30f-411c-b98e-82b84747709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get helper functions\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61631e9c-0608-4198-9af2-aeda92cd3c53",
   "metadata": {},
   "source": [
    "## Get a text dataset\n",
    "\n",
    "The dataset we're using is Kaggle's intro to NLP dataset (text samples of Tweets labelled as disaster or not a disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca881684-6623-4527-a740-27af1be4111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'nlp_getting_started/train.csv'\n",
    "test_dir = 'nlp_getting_started/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0916c211-6afe-420e-b8a5-4b1362225bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv(train_dir)\n",
    "test_data = pd.read_csv(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f59456ac-c163-4497-aa7c-35c7502e0611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8877f1-c815-4b03-8d60-81ea61788208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c3e3aa-45c7-49ce-b020-0bdabfe3c6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shuffle training data\n",
    "train_data_shuffle = train_data.sample(frac=1, random_state=42)\n",
    "train_data_shuffle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc6102-3477-41f1-b0bd-989e20e3c838",
   "metadata": {},
   "source": [
    "## Visualize and become one with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c0889e-d955-4aef-97e7-7000d3229463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class are there?\n",
    "train_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a17a5e6e-7980-45d4-a3a7-2a1a7662fe2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many total samples?\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9f2bfc6-c377-48e6-9253-5f062ebcd7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "China's Stock Market Crash: Are There Gems In The Rubble? http://t.co/Ox3qb15LWQ | https://t.co/8u07FoqjzW http://t.co/tg5fQc8zEY\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Hw18 going 90-100. Dude was keeping up with me. Took the same exit. Pulled to the side and told me he blew his motor. Lolol #2fast2furious\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "@CochiseCollege For the people who died in Human Experiments by Unit 731 of Japanese military http://t.co/vVPLFQv58P http://t.co/ldx9uKNGsk\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "@TANSTAAFL23 It's not an 'impulse' and it doesn't end in mass murder. Correlation does not imply causation.\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "#RoddyPiperAutos Fears over missing migrants in Med: Rescuers search for survivors after a boat carrying as ma...  http://t.co/97B8AVgEWU\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Visualize random training examples\n",
    "import random\n",
    "\n",
    "random_index = random.randint(0, len(train_data)-5)\n",
    "for row in train_data_shuffle[['text','target']][random_index:random_index+5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f'Target: {target}', '(real disaster)' if target > 0 else '(not real disaster)')\n",
    "    print(f'Text:\\n{text}\\n')\n",
    "    print('---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539514cd-18e7-4868-a70f-79daa2dd9eef",
   "metadata": {},
   "source": [
    "### Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8954641b-4f33-49c5-ad83-768e70c39587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_data_shuffle['text'].to_numpy(),\n",
    "                                                                           train_data_shuffle['target'].to_numpy(),\n",
    "                                                                           test_size=.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44d9c603-6c37-4f21-a3b7-c50a47d71b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e730419-088b-4035-9e01-499371ae5a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the first 10 samples\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01e3ca-e01e-40ee-9ec8-b79d81bcdec3",
   "metadata": {},
   "source": [
    "## Converting Text into numbers\n",
    "\n",
    "When Dealing with a text probelm, one of the first things you'll have to do before you can build a model is to convert your text to numbers. \n",
    "\n",
    "There are a few ways to do this:\n",
    "* Tokenization - direct mapping of token (a token could be a word or a character) to number\n",
    "* Embedding - Create a matrix of feature vector for each token (the size of the feature vector can be defined and this embedding can be learned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "834e2a54-452a-4e33-b1af-42d52b61c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b81b7e1-b18a-4943-bb46-52ed3a9830f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the default TextVectorization parameters\n",
    "text_vectorizer = TextVectorization(max_tokens=None, #How many words in the vocabulary (automatically add <OOV>)\n",
    "                                   standardize='lower_and_strip_punctuation', \n",
    "                                   split='whitespace',\n",
    "                                   ngrams=None, #Create groups of n-words\n",
    "                                   output_mode='int', #how to map tokens to numbers\n",
    "                                   output_sequence_length=None, #pads all sequences to the same length, with \"None\" all will get 0's to match the longest sequence\n",
    "                                   pad_to_max_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "691eb0ac-af21-44ce-a599-e21c0f63ede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences[0].split()) #Detects there are 7 words in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efc4aaa5-7eab-410d-80b6-a356031e6663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102087"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the average length of tokens (words) in the training tweets\n",
    "total = round(sum([len(i.split()) for i in train_sentences])) #This gives us the total length of all train sentences in the dataset\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45fa03b2-25bb-4ae7-aec3-56c3d12bfe15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the average length of each tweet we divide by the total number of tweets\n",
    "round(total / len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1245255-1843-47f2-b2a0-d41bf5b6ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Text vectorization variables\n",
    "max_vocab_length = 10000 #Max number of words to have in our vocabulary\n",
    "max_length = 15 #Max length our sequences will be (how many words the model sees)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                   output_mode='int',\n",
    "                                   output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a00103f9-a57b-41c5-9e35-38de9681b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49f17dec-0639-4aa0-8cd6-e61f5158e94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])\n",
    "#We can see the word There's got mapped to 264, a to 3, flood to 232, in to 4, etc. then the 0's is to pad to max length of 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c0a1719-1fe6-4a54-bebb-46aa007cb81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Ready to get annihilated for the BUCS game        \n",
      "\n",
      "Vectorized Version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[924,   5,  52, 558,  10,   2,   1, 397,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Choose a random sentence from the training dataset and tokenize it\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f'Original text:\\n {random_sentence}\\\n",
    "        \\n\\nVectorized Version:')\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bd77869-0688-47c7-af8c-625fe9dbcf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary() #Get all the unique words in the training data\n",
    "top_5_words = words_in_vocab[:5] # Get the most common 5 words\n",
    "bottom_5_words = words_in_vocab[-5:] # Get the least common 5 words\n",
    "\n",
    "print(f'Number of words in vocab: {len(words_in_vocab)}')\n",
    "print(f'5 most common words: {top_5_words}')\n",
    "print(f'5 least common words: {bottom_5_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f554a-3ac9-4c84-bad3-e1ffc92fda6f",
   "metadata": {},
   "source": [
    "* The '' is our empty spaces, [UNK] is the mask for words outside of the 10,000 word vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c0d2a6-a7b0-4e1b-a8db-4857a8e87a4a",
   "metadata": {},
   "source": [
    "### Creating an Embedding using an Embedding Layer\n",
    "\n",
    "To make our embedding, we're going to use Tensorflow's embedding layer\n",
    "\n",
    "The parameters we care most about for our embedding layer:\n",
    "* `input_dim` = the size of our vocabulary\n",
    "* `output_dim` = size of the output embedding vector, for example a value of 100 would mean each token gets represented by a vector 100 long\n",
    "* `input_length` = length of the sequences being passed to the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc472114-57a1-46ce-85a2-850bf791b93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x2da987c50>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim = max_vocab_length, \n",
    "                            output_dim=128, #Setting this to a common number divisible by 8 insures computation speed\n",
    "                            input_length = max_length)\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b77b116-9561-41b9-8201-c169fbbdf9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Monkeys Abused by Notorious Laboratory Dealer | A PETA Eyewitness Invest... https://t.co/QGqlpmRfJd via @YouTube        \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.00446198, -0.01731829, -0.03523778, ...,  0.03687832,\n",
       "          0.0447212 , -0.04217243],\n",
       "        [ 0.04077685,  0.04956594, -0.04109965, ..., -0.02833414,\n",
       "          0.0154579 , -0.0461543 ],\n",
       "        [-0.02807315, -0.01057363, -0.03195492, ...,  0.02459791,\n",
       "          0.01830187,  0.00262801],\n",
       "        ...,\n",
       "        [-0.02387155, -0.00121753, -0.00781842, ...,  0.0155483 ,\n",
       "         -0.0181623 ,  0.03316467],\n",
       "        [ 0.03234914,  0.01103782,  0.03994464, ..., -0.031964  ,\n",
       "          0.01055467, -0.02541409],\n",
       "        [ 0.03234914,  0.01103782,  0.03994464, ..., -0.031964  ,\n",
       "          0.01055467, -0.02541409]]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence form the training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f'Original text:\\n {random_sentence}\\\n",
    "        \\n\\nEmbedded version:') #This takes positive INTEGERS and turns them into embeddings, that is why we must tokenize first because it will not be able to embed straight text\n",
    "\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5bf9916-0c87-4dcb-b766-ed1c6e4041db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monkeys Abused by Notorious Laboratory Dealer | A PETA Eyewitness Invest... https://t.co/QGqlpmRfJd via @YouTube\n",
      "tf.Tensor(\n",
      "[-0.00446198 -0.01731829 -0.03523778 -0.02021638 -0.04750841 -0.03411106\n",
      "  0.00134579  0.0235271  -0.03880454  0.00289973 -0.02729475 -0.04992742\n",
      " -0.03465927 -0.03005945  0.03673089  0.02647002  0.04005947 -0.04114307\n",
      " -0.03851108 -0.01781851 -0.04636509 -0.03519504 -0.01553327  0.03706494\n",
      " -0.03756167  0.01794079  0.04475294 -0.02158301  0.03270828  0.00667913\n",
      "  0.02125063  0.01107174 -0.02021428  0.00571175 -0.0206475   0.0088285\n",
      " -0.04571632 -0.00081696 -0.00645012 -0.02865743  0.02638391 -0.0391278\n",
      " -0.04644766 -0.00393521  0.03841479 -0.03591105 -0.04872149 -0.04291904\n",
      " -0.02825388 -0.00960202 -0.00940876 -0.00591218  0.00291022 -0.04019234\n",
      "  0.04041827 -0.02335994  0.0107852  -0.02897387 -0.02619526  0.02604939\n",
      "  0.00832865  0.01832208  0.03374655 -0.01407516 -0.01128565 -0.02998059\n",
      "  0.03127444 -0.00212105 -0.00816075 -0.00718827 -0.03738066  0.04628411\n",
      " -0.03316522  0.00585599 -0.03558489 -0.04543482  0.01546062 -0.02906398\n",
      "  0.02400768 -0.02296908 -0.01437311  0.02403811 -0.04188478 -0.0290853\n",
      "  0.03949377 -0.00518235  0.01427177 -0.00460721  0.03377639  0.01166239\n",
      " -0.01057935  0.02478132 -0.00069946 -0.00469017  0.02704482  0.01039169\n",
      "  0.03004856 -0.03819789  0.04144217 -0.00814533  0.03713837 -0.01386631\n",
      "  0.00815427 -0.00134776 -0.02354126 -0.04003477 -0.04017014  0.01199989\n",
      " -0.01385745  0.00802344 -0.03472358  0.032632   -0.00547319 -0.02029528\n",
      "  0.04694105  0.01524674 -0.02250607  0.00887283 -0.04193598  0.0450047\n",
      " -0.01248365  0.01228262 -0.04526686 -0.01344775  0.03752512  0.03687832\n",
      "  0.0447212  -0.04217243], shape=(128,), dtype=float32)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "print(random_sentence) #The sentence we are looking at the first word of\n",
    "print(sample_embed[0][0]) #Embedding for a single word\n",
    "print(sample_embed[0][0].shape) #Shape of the embedding for the single word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae52d86-adad-4d7b-adcd-6b92c60d622c",
   "metadata": {},
   "source": [
    "## Modelling a text dataset (Running a series of experiments)\n",
    "### Experiments we're running:\n",
    "\n",
    "* 0: Naive Bayes with TF-IDF encoder (baseline)\n",
    "* 1: Feed-forward Neural Net (Dense Model)\n",
    "* 2: LSTM (RNN)\n",
    "* 3: GRU (RNN)\n",
    "* 4: Bidirectional-LSTM (RNN)\n",
    "* 5: 1D Convolutional Neural Network\n",
    "* 6: TensorFlow Hub Pretrained Feature Extractor\n",
    "* 7: TensorFlow Hub Pretrained Feature Extractor (10% of the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c696840-680e-4f8d-937d-cf6355dea5ed",
   "metadata": {},
   "source": [
    "### Model 0: Getting a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d09ee1c-981b-4775-b604-b7b3484b2a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()), #Convert words to numbers\n",
    "    ('clf', MultinomialNB()) #Model the text\n",
    "])\n",
    "\n",
    "#Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd7dbbe9-d7a0-4f0c-b5f2-251e51b6ed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Achieves an Accuracy of:  79.27%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate our baseline Model\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f'Baseline Model Achieves an Accuracy of: {baseline_score*100: .2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "055f5886-ecec-4f67-b701-a09d0f8ce364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.target.value_counts() #Guessing would be about a 50/50 so the model is outperforming random guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c93a307-8f7f-400d-8b64-26d8f3bfa87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1]\n",
      "Actual Labels: [0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "#Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "print(f'Predicted Labels: {baseline_preds[:20]}')\n",
    "print(f'Actual Labels: {val_labels[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "712f5778-bce5-4f83-be1d-3b66538f0038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/qZQc8WWwcN via @usatoday'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77f09352-0e4d-444f-b874-ad242ee49632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.93      0.83       414\n",
      "           1       0.89      0.63      0.73       348\n",
      "\n",
      "    accuracy                           0.79       762\n",
      "   macro avg       0.82      0.78      0.78       762\n",
      "weighted avg       0.81      0.79      0.79       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(val_labels, baseline_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086010b-02d1-412d-ad0d-7e08c94e8b7e",
   "metadata": {},
   "source": [
    "### Model 1: Feed Forward Neural Network (Dense Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "970d719d-d54f-4a6d-8f04-99c836be08e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280129 (4.88 MB)\n",
      "Trainable params: 1280129 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model with functional API\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string) #Inputs are 1 dimensional strings\n",
    "x = text_vectorizer(inputs) #Tokenize our inputs\n",
    "x = embedding(x) #Turn our tokenized words into embeddings\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x) #Create the output layer for binary outputs\n",
    "\n",
    "model_1 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e66d3383-532f-4683-8289-02ddcc10c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the Model\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "               optimizer = 'adam',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ca21d2e-d75b-4cf5-9f96-594c9a35f486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.6123 - accuracy: 0.6907 - val_loss: 0.5386 - val_accuracy: 0.7467\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.4420 - accuracy: 0.8192 - val_loss: 0.4698 - val_accuracy: 0.7874\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3467 - accuracy: 0.8631 - val_loss: 0.4615 - val_accuracy: 0.7848\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2845 - accuracy: 0.8888 - val_loss: 0.4623 - val_accuracy: 0.7874\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.2377 - accuracy: 0.9105 - val_loss: 0.4802 - val_accuracy: 0.7808\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "model_1_history = model_1.fit(train_sentences, train_labels, epochs=5, validation_data=(val_sentences, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "95515282-06d7-4a60-a772-e9cfd40fb578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 676us/step - loss: 0.4802 - accuracy: 0.7808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48018375039100647, 0.7808399200439453]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f33a0adc-7602-4e4b-90a0-39ddadb95334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 541us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(762, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred = model_1.predict(val_sentences)\n",
    "model_1_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "901d13f7-23b3-4636-808a-cf36a7a1bbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.41930446],\n",
       "        [0.7766302 ],\n",
       "        [0.9977771 ],\n",
       "        [0.13018925],\n",
       "        [0.13156842],\n",
       "        [0.93778   ],\n",
       "        [0.9175396 ],\n",
       "        [0.9933553 ],\n",
       "        [0.9711619 ],\n",
       "        [0.3107521 ]], dtype=float32),\n",
       " array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the first 10 predictions\n",
    "model_1_pred[:10], val_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db06b7f0-8eca-41ca-8f3c-7f3566701818",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_pred_converted = tf.squeeze(tf.round(model_1_pred)) #This will turn predictions into 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5ebfb25f-cb70-4d50-b74a-f6379143ef3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>,\n",
       " array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred_converted[:10], val_labels[:10] #Confirm they are in the same format and peek at the first 10 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3d37cd7d-8ac4-49de-9db6-fee2a522c51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.86      0.81       414\n",
      "           1       0.80      0.69      0.74       348\n",
      "\n",
      "    accuracy                           0.78       762\n",
      "   macro avg       0.79      0.77      0.78       762\n",
      "weighted avg       0.78      0.78      0.78       762\n",
      "\n",
      "Our baseline appears to be outperforming the first deep learning model\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, model_1_pred_converted))\n",
    "print('Our baseline appears to be outperforming the first deep learning model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb80b0-ba94-4550-a238-576fadadedf1",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN's)\n",
    "\n",
    "RNN's are useful for sequence data\n",
    "\n",
    "The premise of a recurrent neural net is to use the representation of a previous input to aid the representation of a later input\n",
    "\n",
    "for an overview of the internals of a recurrent neural network see:\n",
    "\n",
    "* MIT's Sequence Modelling lecture: https://www.youtube.com/watch?v=ySEx_Bqxvvo&t=18s\n",
    "* Chris Olah's intro to LSTM's: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* Andrej Karpathy's \"The Unreasonable Effectiveness of Recurrent Neural Networks\": https://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e02e43-d1b1-4f4d-ac6e-6983c59d4d7a",
   "metadata": {},
   "source": [
    "### Model 2: LSTM\n",
    "\n",
    "LSTM = Long short term memory (one of the most popular LSTM cells)\n",
    "\n",
    "Our structure of an RNN typically looks like this:\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers (RNNs/Dense) -> Output (label probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4dad6ba0-68ac-42be-b741-fe7a07801a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LSTM Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "# print(x.shape)\n",
    "x = layers.LSTM(64, return_sequences=True)(x) #When stacking RNN cells together, need to set return_sequences = True\n",
    "# print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "# print(x.shape)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_2 = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "feb8e964-a228-4548-945a-122cfcf949dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 15, 64)            49408     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1366657 (5.21 MB)\n",
      "Trainable params: 1366657 (5.21 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get a summary\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4491146b-08a5-4fbe-9f85-58bd7b390c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model_2.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer = tf.keras.optimizers.legacy.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33c0397c-56e8-49a6-a122-0192b1083f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 16ms/step - loss: 0.1398 - accuracy: 0.9574 - val_loss: 0.9258 - val_accuracy: 0.7717\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0769 - accuracy: 0.9711 - val_loss: 1.1354 - val_accuracy: 0.7585\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0619 - accuracy: 0.9750 - val_loss: 1.3431 - val_accuracy: 0.7730\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0562 - accuracy: 0.9756 - val_loss: 1.2403 - val_accuracy: 0.7651\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0618 - accuracy: 0.9724 - val_loss: 1.3587 - val_accuracy: 0.7677\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "model_2_history = model_2.fit(train_sentences, train_labels, epochs=5, validation_data=(val_sentences, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22037efa-3557-4526-9319-44dadee35e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.4498703e-03],\n",
       "       [5.0837511e-01],\n",
       "       [9.9993849e-01],\n",
       "       [1.7495912e-01],\n",
       "       [2.1392272e-05],\n",
       "       [9.9975646e-01],\n",
       "       [9.8994821e-01],\n",
       "       [9.9995428e-01],\n",
       "       [9.9993575e-01],\n",
       "       [7.2909915e-01]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_pred = model_2.predict(val_sentences)\n",
    "model_2_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b25e1bd3-67d5-4789-a7f6-373b1a0b2869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert to predictions\n",
    "model_2_predictions = tf.squeeze(tf.round(model_2_pred))\n",
    "model_2_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3a24207e-552a-418c-bce3-a1bc715c4e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.84      0.80       414\n",
      "           1       0.78      0.68      0.73       348\n",
      "\n",
      "    accuracy                           0.77       762\n",
      "   macro avg       0.77      0.76      0.76       762\n",
      "weighted avg       0.77      0.77      0.77       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, model_2_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ba300-5942-49db-b223-90444520a033",
   "metadata": {},
   "source": [
    "### Model 3: GRU powered RNN\n",
    "\n",
    "Another popular and effective RNN component is the GRU or Gated Recurrent Unit.\n",
    "\n",
    "The GRU cell has similar features to an LSTM cell but has less parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4ad6fbcc-5ba2-41b4-b1e8-00ae4f0b4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GRU(64, return_sequences=True)(x)\n",
    "x = layers.LSTM(64, return_sequences=True)(x)\n",
    "x = layers.GRU(64, return_sequences=True)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_3 = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cacc1dfb-4fd3-48c6-819a-017abd1932f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru_10 (GRU)                (None, 15, 64)            37248     \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 15, 64)            33024     \n",
      "                                                                 \n",
      " gru_11 (GRU)                (None, 15, 64)            24960     \n",
      "                                                                 \n",
      " global_average_pooling1d_2  (None, 64)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1379457 (5.26 MB)\n",
      "Trainable params: 1379457 (5.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "92c46ad6-131b-4c28-aab5-b9c6fabe5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer = tf.keras.optimizers.legacy.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c23358fc-0cef-4d22-b725-4073822728ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 5s 17ms/step - loss: 0.1199 - accuracy: 0.9571 - val_loss: 1.1914 - val_accuracy: 0.7703\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0514 - accuracy: 0.9765 - val_loss: 1.2182 - val_accuracy: 0.7703\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0457 - accuracy: 0.9785 - val_loss: 1.5062 - val_accuracy: 0.7664\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 16ms/step - loss: 0.0441 - accuracy: 0.9796 - val_loss: 1.6337 - val_accuracy: 0.7651\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0400 - accuracy: 0.9813 - val_loss: 1.9059 - val_accuracy: 0.7651\n"
     ]
    }
   ],
   "source": [
    "model_3_history = model_3.fit(train_sentences, train_labels, epochs=5, validation_data=(val_sentences, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "29e8f7e7-2f76-4816-90ba-27f93c6f668b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.9681400e-02],\n",
       "       [4.9318841e-01],\n",
       "       [9.9999249e-01],\n",
       "       [1.2864037e-01],\n",
       "       [4.5325450e-08],\n",
       "       [9.9994177e-01],\n",
       "       [9.9937451e-01],\n",
       "       [9.9999994e-01],\n",
       "       [9.9999958e-01],\n",
       "       [9.9739516e-01]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8f763787-a6dd-45a6-a7ea-e31c8c374e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4d26805d-11ae-4cdd-9d47-b6eca808f7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.84      0.79       414\n",
      "           1       0.78      0.68      0.73       348\n",
      "\n",
      "    accuracy                           0.77       762\n",
      "   macro avg       0.77      0.76      0.76       762\n",
      "weighted avg       0.77      0.77      0.76       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, model_3_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1803f6-4604-4a7d-8a33-8976b9f11d26",
   "metadata": {},
   "source": [
    "### Model 4: Bidirectional LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2be200d4-d3ea-4e17-9c74-1286992303dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences = True))(x)\n",
    "x = layers.Bidirectional(layers.GRU(64, return_sequences=True))(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_4 = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3617c3c2-d5a9-49e1-87ab-7fa055be8969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirecti  (None, 15, 128)           98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirecti  (None, 15, 128)           74496     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " global_average_pooling1d_4  (None, 128)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1461633 (5.58 MB)\n",
      "Trainable params: 1461633 (5.58 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ad6f3c30-808b-4818-8030-0b3b85e360ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4b39d70a-bd09-411a-94f5-26e4849f349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 6s 17ms/step - loss: 0.0753 - accuracy: 0.9745 - val_loss: 1.5306 - val_accuracy: 0.7441\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 16ms/step - loss: 0.0362 - accuracy: 0.9816 - val_loss: 1.2548 - val_accuracy: 0.7559\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.0346 - accuracy: 0.9823 - val_loss: 1.7109 - val_accuracy: 0.7559\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0371 - accuracy: 0.9810 - val_loss: 1.6832 - val_accuracy: 0.7612\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 3s 15ms/step - loss: 0.0332 - accuracy: 0.9825 - val_loss: 1.9600 - val_accuracy: 0.7585\n"
     ]
    }
   ],
   "source": [
    "model_4_history = model_4.fit(train_sentences, train_labels, epochs=5, validation_data=(val_sentences, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "675b5fc7-d51a-4c44-b386-e3336419ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.9248557e-01],\n",
       "       [4.9983552e-01],\n",
       "       [9.9999583e-01],\n",
       "       [3.7908199e-01],\n",
       "       [2.0135833e-07],\n",
       "       [9.9998653e-01],\n",
       "       [9.9950999e-01],\n",
       "       [9.9999958e-01],\n",
       "       [9.9999827e-01],\n",
       "       [9.9134916e-01]], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9a0f5802-385a-4344-ad44-bed9c9ae6e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 0., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2f6b481d-69f1-4306-9cdc-02859bac41be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.79       414\n",
      "           1       0.76      0.68      0.72       348\n",
      "\n",
      "    accuracy                           0.76       762\n",
      "   macro avg       0.76      0.75      0.75       762\n",
      "weighted avg       0.76      0.76      0.76       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, model_4_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79b85f0-d6f8-4957-90ce-72d6df04e938",
   "metadata": {},
   "source": [
    "### Model 5: Conv1D layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cb888b51-0538-42f4-9adf-48a5ba8141ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_5 = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8cfb917e-21cf-4248-8e31-51ff8f982b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_35 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d_47 (Conv1D)          (None, 11, 64)            41024     \n",
      "                                                                 \n",
      " global_max_pooling1d_23 (G  (None, 64)                0         \n",
      " lobalMaxPooling1D)                                              \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1329473 (5.07 MB)\n",
      "Trainable params: 1329473 (5.07 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e0f414eb-3058-41ee-b6d9-fd6025848cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "54548f51-a4cb-4fc0-af89-0e00e5ca637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.0707 - accuracy: 0.9746 - val_loss: 1.1749 - val_accuracy: 0.7336\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.0322 - accuracy: 0.9835 - val_loss: 1.5186 - val_accuracy: 0.7349\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.0282 - accuracy: 0.9850 - val_loss: 1.7531 - val_accuracy: 0.7283\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.0273 - accuracy: 0.9854 - val_loss: 1.8142 - val_accuracy: 0.7231\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.0264 - accuracy: 0.9848 - val_loss: 2.2180 - val_accuracy: 0.7323\n"
     ]
    }
   ],
   "source": [
    "model_5_history = model_5.fit(train_sentences, train_labels, epochs=5, validation_data=(val_sentences, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6e085c8c-d2e3-47e3-8a1e-7f3245f35b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 806us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.6750293e-01],\n",
       "       [9.9913502e-01],\n",
       "       [1.0000000e+00],\n",
       "       [3.9535664e-02],\n",
       "       [1.4018751e-11],\n",
       "       [9.9999905e-01],\n",
       "       [9.9986297e-01],\n",
       "       [1.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       [9.5057124e-01]], dtype=float32)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "82d34eef-1437-4b36-bb5a-c374b9697cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a0ea4df6-ad33-4de5-8b23-dc57be927348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76       414\n",
      "           1       0.71      0.69      0.70       348\n",
      "\n",
      "    accuracy                           0.73       762\n",
      "   macro avg       0.73      0.73      0.73       762\n",
      "weighted avg       0.73      0.73      0.73       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, model_5_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b7cc4-ad23-4331-a1aa-b54046442428",
   "metadata": {},
   "source": [
    "### Model 6: TensorFlow Hub Pretrained Feature Extractor (Sentence Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "27677c36-1020-4b4f-a425-402764fd650e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01157025  0.02485908  0.0287805  -0.01271502  0.03971539  0.0882776\n",
      "  0.02680984  0.05589836 -0.01068733 -0.00597292  0.00639322 -0.01819517\n",
      "  0.00030816  0.09105889  0.05874644 -0.03180627  0.01512474 -0.05162928\n",
      "  0.00991366 -0.06865346 -0.04209306  0.02678981  0.03011009  0.00321063\n",
      " -0.00337969 -0.04787361  0.02266721 -0.00985927 -0.04063616 -0.01292092\n",
      " -0.04666384  0.05630298 -0.03949255  0.00517684  0.02495823 -0.07014441\n",
      "  0.02871507  0.0494768  -0.00633974 -0.08960193  0.0280712  -0.00808362\n",
      " -0.01360601  0.0599865  -0.10361787 -0.05195376  0.00232959 -0.02332532\n",
      " -0.03758109  0.03327732], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "embed = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')\n",
    "embed_samples = embed([sample_sentence, \"When you can the universal sentence encoder on a sentence, it turns into numbers.\"])\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "66dec8f7-d47f-4729-9de0-f0bbc132ab7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There's a flood in my street!\""
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8844e6c6-a9e0-46c8-a118-cd57059260e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_samples[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ee07e574-e152-435d-814c-6d5715f4b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Keras layer using the USE pretrained llayers\n",
    "sentence_encoder_layer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4',\n",
    "                                        input_shape=[],\n",
    "                                        dtype = tf.string,\n",
    "                                        trainable=False,\n",
    "                                       name='USE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d6e08363-4175-4346-8587-0786f99363a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model using Sequential API\n",
    "model_6 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    # layers.GlobalMaxPool1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a93053e0-1875-4748-8e8c-073242ffe24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256863617 (979.86 MB)\n",
      "Trainable params: 65793 (257.00 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_6.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ac519752-9523-4643-bb3f-93dfe95528ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 1s 5ms/step - loss: 0.4907 - accuracy: 0.7869 - val_loss: 0.4448 - val_accuracy: 0.7966\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 1s 3ms/step - loss: 0.4090 - accuracy: 0.8183 - val_loss: 0.4386 - val_accuracy: 0.8071\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3948 - accuracy: 0.8247 - val_loss: 0.4263 - val_accuracy: 0.8176\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3844 - accuracy: 0.8313 - val_loss: 0.4266 - val_accuracy: 0.8163\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 4ms/step - loss: 0.3745 - accuracy: 0.8335 - val_loss: 0.4233 - val_accuracy: 0.8202\n"
     ]
    }
   ],
   "source": [
    "model_6_history = model_6.fit(train_sentences, train_labels, epochs=5, validation_data=(val_sentences, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "deef2d1b-89dd-451d-833f-e3ea09fee26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.14202861],\n",
       "       [0.7695492 ],\n",
       "       [0.9922468 ],\n",
       "       [0.22077411],\n",
       "       [0.61421853],\n",
       "       [0.75251126],\n",
       "       [0.98547614],\n",
       "       [0.9786936 ],\n",
       "       [0.95257086],\n",
       "       [0.09724431]], dtype=float32)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_pred_probs = model_6.predict(val_sentences)\n",
    "model_6_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ce6ecbe6-8504-4ae8-aef0-6b07e4a001af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\n",
    "model_6_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "513b47f5-bbcd-4e11-aa0e-20db1f7050b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84       414\n",
      "           1       0.85      0.74      0.79       348\n",
      "\n",
      "    accuracy                           0.82       762\n",
      "   macro avg       0.83      0.81      0.82       762\n",
      "weighted avg       0.82      0.82      0.82       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, model_6_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b28d40b-47a6-46bb-ba29-0dfa777bd05a",
   "metadata": {},
   "source": [
    "### Model 7: TF Hub Pretrained USE but with only 10% of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8c28efb6-de37-47e5-a4d2-b101ebfe0382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So you have a new weapon that can cause un-imaginable destruction.'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len = int(0.1 * len(train_sentences))\n",
    "train_sentences_10_percent = train_data_shuffle['text'][:train_len].to_list()\n",
    "train_sentences_10_percent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ca397fb9-d0e5-413f-a727-9bd7d2422f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_10_percent = train_data_shuffle['target'][:train_len].to_list()\n",
    "train_label_10_percent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2ae0fb7a-4ac9-4d4d-8d4a-0867fb0eeb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(685, 685)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label_10_percent), len(train_sentences_10_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "436f1838-7c26-4aa4-a475-695c7787ca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "0670ec06-0056-45e8-9531-1fb6d5885cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "897a1873-2878-44a4-aec2-3c84d2aa7911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " USE (KerasLayer)            (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256863617 (979.86 MB)\n",
      "Trainable params: 65793 (257.00 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_7.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "dc769e5f-78d2-4312-83ed-110e374f85c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Excited for Cyclone football https://t.co/Xqv6gzZMmN'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The training data came from 10% of train df shuffled but so did the validation data! We could have an overlap of data, lets change val_sentences and labels\n",
    "val_sentences = train_data_shuffle['text'][train_len:].to_list()\n",
    "val_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f94c7b0c-d0fc-4e4d-bc13-49350f851b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels = train_data_shuffle['target'][train_len:].to_list()\n",
    "val_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a5a4ee20-0fa1-4848-b772-68c6538820d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 0.2096 - accuracy: 0.9241 - val_loss: 0.5137 - val_accuracy: 0.7777\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.1982 - accuracy: 0.9270 - val_loss: 0.5216 - val_accuracy: 0.7774\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.1864 - accuracy: 0.9387 - val_loss: 0.5300 - val_accuracy: 0.7761\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.1767 - accuracy: 0.9416 - val_loss: 0.5390 - val_accuracy: 0.7745\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.1668 - accuracy: 0.9518 - val_loss: 0.5455 - val_accuracy: 0.7760\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 0.1601 - accuracy: 0.9518 - val_loss: 0.5554 - val_accuracy: 0.7724\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.1484 - accuracy: 0.9650 - val_loss: 0.5660 - val_accuracy: 0.7724\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 0.1387 - accuracy: 0.9679 - val_loss: 0.5740 - val_accuracy: 0.7721\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 0.1303 - accuracy: 0.9737 - val_loss: 0.5837 - val_accuracy: 0.7717\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 1s 36ms/step - loss: 0.1233 - accuracy: 0.9766 - val_loss: 0.5964 - val_accuracy: 0.7689\n"
     ]
    }
   ],
   "source": [
    "model_7_history = model_7.fit(train_sentences_10_percent, train_label_10_percent, epochs=10, validation_data=(val_sentences, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "dc345c53-d988-4665-ba37-9973a27f0e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217/217 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.09672087],\n",
       "       [0.997689  ],\n",
       "       [0.7209684 ],\n",
       "       [0.77340657],\n",
       "       [0.9174134 ],\n",
       "       [0.34599334],\n",
       "       [0.9923149 ],\n",
       "       [0.12435134],\n",
       "       [0.9493512 ],\n",
       "       [0.8939466 ]], dtype=float32)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7_pred_probs = model_7.predict(val_sentences)\n",
    "model_7_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "eb9b0a58-f1e1-4a7b-a823-d760d46aa997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 1., 1., 0., 1., 0., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
    "model_7_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "0c33fc1e-effa-4096-b522-8215b64a9716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.80      3960\n",
      "           1       0.72      0.75      0.74      2968\n",
      "\n",
      "    accuracy                           0.77      6928\n",
      "   macro avg       0.76      0.77      0.77      6928\n",
      "weighted avg       0.77      0.77      0.77      6928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, model_7_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526b43c4-e1dd-427a-b737-9eb31b1b5d2e",
   "metadata": {},
   "source": [
    "### Model 8: Making layers in pretrained model trainable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b4dcc7a0-cf20-479d-94a8-13487f029fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "215/215 [==============================] - 138s 630ms/step - loss: 0.4705 - accuracy: 0.8321 - val_loss: 0.4284 - val_accuracy: 0.8136\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 83s 383ms/step - loss: 0.3211 - accuracy: 0.8698 - val_loss: 0.4240 - val_accuracy: 0.8136\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 90s 418ms/step - loss: 0.2577 - accuracy: 0.8961 - val_loss: 0.4334 - val_accuracy: 0.8215\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 80s 374ms/step - loss: 0.1970 - accuracy: 0.9264 - val_loss: 0.4577 - val_accuracy: 0.8202\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 85s 397ms/step - loss: 0.1358 - accuracy: 0.9537 - val_loss: 0.5094 - val_accuracy: 0.8163\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 87s 407ms/step - loss: 0.0860 - accuracy: 0.9710 - val_loss: 0.6110 - val_accuracy: 0.7822\n"
     ]
    }
   ],
   "source": [
    "sentence_encoder_layer.trainable = True\n",
    "\n",
    "model_8 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_8.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer = tf.keras.optimizers.legacy.Adam(.0001))\n",
    "\n",
    "model_8_history = model_8.fit(train_sentences, train_labels, epochs=10, initial_epoch=model_6_history.epoch[-1], validation_data=(val_sentences, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c644a93e-7f15-4be2-b7a1-79a50a172ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03602282],\n",
       "       [0.9423729 ],\n",
       "       [0.99671674],\n",
       "       [0.08216799],\n",
       "       [0.9304532 ],\n",
       "       [0.99275005],\n",
       "       [0.9961126 ],\n",
       "       [0.99662423],\n",
       "       [0.9954866 ],\n",
       "       [0.04435186]], dtype=float32)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_8_pred_probs = model_8.predict(val_sentences)\n",
    "model_8_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8f15ad89-06e1-4c6e-834c-1cb494a7da45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_8_preds = tf.squeeze(tf.round(model_8_pred_probs))\n",
    "model_8_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4e66d892-25b4-41bc-9de5-793c481b63e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.76      0.79       414\n",
      "           1       0.74      0.81      0.77       348\n",
      "\n",
      "    accuracy                           0.78       762\n",
      "   macro avg       0.78      0.78      0.78       762\n",
      "weighted avg       0.79      0.78      0.78       762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, model_8_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916f2f8-16bf-42e9-a6e9-0cac33f47506",
   "metadata": {},
   "source": [
    "## The Best model was model 6\n",
    "\n",
    "The results make me feel as though there is a cap on how well the model can do due to the labels on the tweets. We did see some were mislabled when visualizing before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "d22c526a-7fa1-4314-a4d7-4b2fead895d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_6_pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "0daba0ee-65d9-4d8b-b70a-51447e4d1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = []\n",
    "model_6_pred_squeeze = tf.squeeze(model_6_pred_probs)\n",
    "for i in range(len(model_6_pred_probs)):\n",
    "    pred = model_6_pred_squeeze[i].numpy()\n",
    "    if pred < 0.5:\n",
    "        pred = 1 - pred\n",
    "    pred = round(pred * 100, 2)\n",
    "    confidence.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "382a5949-878f-4539-9bfc-4d9847a5c068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.8"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "d4e47f3d-2dad-4ad0-b550-99d0a9c0b787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DFR EP016 Monthly Meltdown - On Dnbheaven 2015...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FedEx no longer to transport bioterror germs i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gunmen kill four in El Salvador bus attack: Su...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@camilacabello97 Internally and externally scr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radiation emergency #preparedness starts with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>That's the ultimate road to destruction</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>@SetZorah dad why dont you claim me that mean ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>FedEx will no longer transport bioterror patho...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>Crack in the path where I wiped out this morni...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>I liked a @YouTube video from @dannyonpc http:...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>762 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label  pred  \\\n",
       "0    DFR EP016 Monthly Meltdown - On Dnbheaven 2015...      0   0.0   \n",
       "1    FedEx no longer to transport bioterror germs i...      0   1.0   \n",
       "2    Gunmen kill four in El Salvador bus attack: Su...      1   1.0   \n",
       "3    @camilacabello97 Internally and externally scr...      1   0.0   \n",
       "4    Radiation emergency #preparedness starts with ...      1   1.0   \n",
       "..                                                 ...    ...   ...   \n",
       "757            That's the ultimate road to destruction      0   0.0   \n",
       "758  @SetZorah dad why dont you claim me that mean ...      0   0.0   \n",
       "759  FedEx will no longer transport bioterror patho...      0   1.0   \n",
       "760  Crack in the path where I wiped out this morni...      0   1.0   \n",
       "761  I liked a @YouTube video from @dannyonpc http:...      0   0.0   \n",
       "\n",
       "     confidence  \n",
       "0         85.80  \n",
       "1         76.95  \n",
       "2         99.22  \n",
       "3         77.92  \n",
       "4         61.42  \n",
       "..          ...  \n",
       "757       87.80  \n",
       "758       88.24  \n",
       "759       86.99  \n",
       "760       63.87  \n",
       "761       88.20  \n",
       "\n",
       "[762 rows x 4 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'sentence': val_sentences.tolist(), 'label': val_labels.tolist(), 'pred': model_6_preds, 'confidence': confidence})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3f150125-280e-4ea5-a08f-c52301166be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "      <th>confidence</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DFR EP016 Monthly Meltdown - On Dnbheaven 2015...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.80</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FedEx no longer to transport bioterror germs i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.95</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gunmen kill four in El Salvador bus attack: Su...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.22</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@camilacabello97 Internally and externally scr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.92</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radiation emergency #preparedness starts with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.42</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  pred  confidence  \\\n",
       "0  DFR EP016 Monthly Meltdown - On Dnbheaven 2015...      0   0.0       85.80   \n",
       "1  FedEx no longer to transport bioterror germs i...      0   1.0       76.95   \n",
       "2  Gunmen kill four in El Salvador bus attack: Su...      1   1.0       99.22   \n",
       "3  @camilacabello97 Internally and externally scr...      1   0.0       77.92   \n",
       "4  Radiation emergency #preparedness starts with ...      1   1.0       61.42   \n",
       "\n",
       "   correct  \n",
       "0     True  \n",
       "1    False  \n",
       "2     True  \n",
       "3    False  \n",
       "4     True  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['correct'] = df['label'] == df['pred']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "9e5a2d03-eea0-474a-9574-ecf55ecfb27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "      <th>confidence</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Why are you deluged with low self-image? Take the quiz: http://t.co/XsPqdOrIqj http://t.co/CQYvFR4UCy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.26</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>@SoonerMagic_ I mean I'm a fan but I don't need a girl sounding off like a damn siren</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>I get to smoke my shit in peace</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.96</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ron &amp;amp; Fez - Dave's High School Crush https://t.co/aN3W16c8F6 via @YouTube</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.92</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Reddit Will Now QuarantineÛ_ http://t.co/pkUAMXw6pm #onlinecommunities #reddit #amageddon #freespeech #Business http://t.co/PAWvNJ4sAP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>You can never escape me. Bullets don't harm me. Nothing harms me. But I know pain. I know pain. Sometimes I share it. With someone like you.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.58</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>'The way you move is like a full on rainstorm and I'm a house of cards'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.05</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Lucas Duda is Ghost Rider. Not the Nic Cage version but an actual 'engulfed in flames' badass. #Mets</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.97</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>@DavidVonderhaar At least you were sincere ??</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.75</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>VICTORINOX SWISS ARMY DATE WOMEN'S RUBBER MOP WATCH 241487 http://t.co/yFy3nkkcoH http://t.co/KNEhVvOHVK</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.71</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>@willienelson We need help! Horses will die!Please RT &amp;amp; sign petition!Take a stand &amp;amp; be a voice for them! #gilbert23 https://t.co/e8dl1lNCVu</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.40</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>@BoyInAHorsemask its a panda trapped in a dogs body</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.39</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>going to redo my nails and watch behind the scenes of desolation of smaug ayyy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.99</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>New post from @darkreading http://t.co/8eIJDXApnp New SMB Relay Attack Steals User Credentials Over Internet</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.93</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Next May I'll be free...from school from obligations like family.... Best of all that damn curfew...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.89</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Perspectives on the Grateful Dead: Critical Writings (Contributions to the Study http://t.co/fmu0fnuMxf http://t.co/AgGRyhVXKr</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.93</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>? High Skies - Burning Buildings ? http://t.co/uVq41i3Kx2 #nowplaying</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>91.68</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>If I fall is men GOD @Praiz8 is d bomb well av always known dat since 2008 bigger u I pray sir</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.64</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>I Will Survive by Gloria Gaynor (with Oktaviana Devi) ÛÓ https://t.co/HUkJZ1wT36</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.47</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>Rand Paul's Debate Strategy 'demolish Some other bad ideas out there or point out maybe that there are some em... http://t.co/qzdqRBr4Lh</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.37</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                 sentence  \\\n",
       "38                                                  Why are you deluged with low self-image? Take the quiz: http://t.co/XsPqdOrIqj http://t.co/CQYvFR4UCy   \n",
       "411                                                                 @SoonerMagic_ I mean I'm a fan but I don't need a girl sounding off like a damn siren   \n",
       "233                                                                                                                       I get to smoke my shit in peace   \n",
       "23                                                                          Ron &amp; Fez - Dave's High School Crush https://t.co/aN3W16c8F6 via @YouTube   \n",
       "244               Reddit Will Now QuarantineÛ_ http://t.co/pkUAMXw6pm #onlinecommunities #reddit #amageddon #freespeech #Business http://t.co/PAWvNJ4sAP   \n",
       "59           You can never escape me. Bullets don't harm me. Nothing harms me. But I know pain. I know pain. Sometimes I share it. With someone like you.   \n",
       "681                                                                               'The way you move is like a full on rainstorm and I'm a house of cards'   \n",
       "294                                                  Lucas Duda is Ghost Rider. Not the Nic Cage version but an actual 'engulfed in flames' badass. #Mets   \n",
       "536                                                                                                         @DavidVonderhaar At least you were sincere ??   \n",
       "486                                              VICTORINOX SWISS ARMY DATE WOMEN'S RUBBER MOP WATCH 241487 http://t.co/yFy3nkkcoH http://t.co/KNEhVvOHVK   \n",
       "408  @willienelson We need help! Horses will die!Please RT &amp; sign petition!Take a stand &amp; be a voice for them! #gilbert23 https://t.co/e8dl1lNCVu   \n",
       "40                                                                                                    @BoyInAHorsemask its a panda trapped in a dogs body   \n",
       "221                                                                        going to redo my nails and watch behind the scenes of desolation of smaug ayyy   \n",
       "192                                          New post from @darkreading http://t.co/8eIJDXApnp New SMB Relay Attack Steals User Credentials Over Internet   \n",
       "361                                                  Next May I'll be free...from school from obligations like family.... Best of all that damn curfew...   \n",
       "198                        Perspectives on the Grateful Dead: Critical Writings (Contributions to the Study http://t.co/fmu0fnuMxf http://t.co/AgGRyhVXKr   \n",
       "31                                                                                  ? High Skies - Burning Buildings ? http://t.co/uVq41i3Kx2 #nowplaying   \n",
       "261                                                        If I fall is men GOD @Praiz8 is d bomb well av always known dat since 2008 bigger u I pray sir   \n",
       "63                                                                      I Will Survive by Gloria Gaynor (with Oktaviana Devi) ÛÓ https://t.co/HUkJZ1wT36   \n",
       "535              Rand Paul's Debate Strategy 'demolish Some other bad ideas out there or point out maybe that there are some em... http://t.co/qzdqRBr4Lh   \n",
       "\n",
       "     label  pred  confidence  correct  \n",
       "38       1   0.0       97.26    False  \n",
       "411      1   0.0       96.10    False  \n",
       "233      1   0.0       95.96    False  \n",
       "23       1   0.0       95.92    False  \n",
       "244      1   0.0       95.01    False  \n",
       "59       1   0.0       94.58    False  \n",
       "681      1   0.0       94.05    False  \n",
       "294      1   0.0       93.97    False  \n",
       "536      1   0.0       93.75    False  \n",
       "486      1   0.0       93.71    False  \n",
       "408      1   0.0       93.40    False  \n",
       "40       1   0.0       93.39    False  \n",
       "221      1   0.0       92.99    False  \n",
       "192      1   0.0       92.93    False  \n",
       "361      1   0.0       92.89    False  \n",
       "198      1   0.0       91.93    False  \n",
       "31       0   1.0       91.68    False  \n",
       "261      1   0.0       91.64    False  \n",
       "63       1   0.0       90.47    False  \n",
       "535      1   0.0       90.37    False  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "top_wrong = df[df['correct']==False].sort_values('confidence', ascending=False)\n",
    "top_wrong.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25402770-e8c2-43d4-a51a-fb2b67df71b8",
   "metadata": {},
   "source": [
    "### We can obviously see that many of these the model is confident about are labeled disaster but are not in actuality disasters, many are mislabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "50e7694a-db0d-4184-b838-0f4fd5510a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTENERS XrWn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city&amp;amp;3others hardest hit. My yard looks like it was bombed. Around 20000K still without power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/UtbXLcBIuY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) http://t.co/3X6RBQJHn3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Emergency Plan. #yycstorm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                                                                                                             text  \n",
       "0                                                                                                              Just happened a terrible car crash  \n",
       "1                                                                                Heard about #earthquake is different cities, stay safe everyone.  \n",
       "2                                                there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all  \n",
       "3                                                                                                        Apocalypse lighting. #Spokane #wildfires  \n",
       "4                                                                                                   Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                                                                                                           ...  \n",
       "3258                                                                                      EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTENERS XrWn  \n",
       "3259  Storm in RI worse than last hurricane. My city&amp;3others hardest hit. My yard looks like it was bombed. Around 20000K still without power  \n",
       "3260                                                                                      Green Line derailment in Chicago http://t.co/UtbXLcBIuY  \n",
       "3261                                                                            MEG issues Hazardous Weather Outlook (HWO) http://t.co/3X6RBQJHn3  \n",
       "3262                                                                         #CityofCalgary has activated its Municipal Emergency Plan. #yycstorm  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets compare our results onto the test df!\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b5795bb5-5ee7-4e89-9b63-2c8071faf14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data['text'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "b7a3e0b2-75bf-4203-929d-2aaab6b1feb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9731597 ],\n",
       "       [0.9933057 ],\n",
       "       [0.9819086 ],\n",
       "       [0.91518265],\n",
       "       [0.9266316 ],\n",
       "       [0.93765   ],\n",
       "       [0.03278901],\n",
       "       [0.02341782],\n",
       "       [0.01728554],\n",
       "       [0.02358675]], dtype=float32)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_test_pred_prob = model_6.predict(test_sentences)\n",
    "model_6_test_pred_prob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "48a88287-3173-4fde-b054-a61d0c0ab6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_test = []\n",
    "model_6_pred_squeeze = tf.squeeze(model_6_test_pred_prob)\n",
    "for i in range(len(model_6_test_pred_prob)):\n",
    "    pred = model_6_pred_squeeze[i].numpy()\n",
    "    if pred < 0.5:\n",
    "        pred = 1 - pred\n",
    "    pred = round(pred * 100, 2)\n",
    "    confidence_test.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "13f46c34-0e2b-43b6-ba12-0aee58eca3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3263,), dtype=float32, numpy=array([1., 1., 1., ..., 1., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_pred = tf.squeeze(tf.round(model_6_test_pred_prob))\n",
    "model_6_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ed3a406f-387d-42bc-a861-d68103c068b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Pred Label</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1.0</td>\n",
       "      <td>97.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1.0</td>\n",
       "      <td>91.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>They'd probably still show more life than Arsenal did yesterday, eh? EH?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hey! How are you?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What a nice hat?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fuck off!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>No I don't like cold!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NOOOOOOOOO! Don't do that!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>No don't tell me that!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What if?!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Awesome!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Birmingham Wholesale Market is ablaze BBC News - Fire breaks out at Birmingham's Wholesale Market http://t.co/irWqCEZWEU</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@sunkxssedharry will you wear shorts for race ablaze ?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>#PreviouslyOnDoyinTv: Toke MakinwaÛªs marriage crisis sets Nigerian Twitter ablaze... http://t.co/CMghxBa2XI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http://t.co/3Tj8ZjiN21 http://t.co/YDUiXEfIpE http://t.co/LxTjc87KLS #nsfw</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PSA: IÛªm splitting my personalities.\\n\\n?? techies follow @ablaze_co\\n?? Burners follow @ablaze</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>beware world ablaze sierra leone &amp;amp; guap.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Burning Man Ablaze! by Turban Diva http://t.co/hodWosAmWS via @Etsy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Not a diss song. People will take 1 thing and run with it. Smh it's an eye opener though. He is about 2 set the game ablaze @CyhiThePrynce</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Rape victim dies as she sets herself ablaze: A 16-year-old girl died of burn injuries as she set herself ablazeÛ_ http://t.co/UK8hNrbOob</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SETTING MYSELF ABLAZE http://t.co/6vMe7P5XhC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>@CTVToronto the bins in front of the field by my house wer set ablaze the other day flames went rite up the hydro pole wonder if it was him</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>#nowplaying Alfons - Ablaze 2015 on Puls Radio #pulsradio http://t.co/aA5BJgWfDv</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>'Burning Rahm': Let's hope City Hall builds a giant wooden mayoral effigy 100 feet tall &amp;amp; sets it ablaze. http://t.co/kFo2mksn6Y @John_Kass</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@PhilippaEilhart @DhuBlath hurt but her eyes ablaze with insulted anger.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Accident cleared in #PaTurnpike on PATP EB between PA-18 and Cranberry slow back to #traffic http://t.co/SL0Oqn0Vyr</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Just got to love burning your self on a damn curling wand... I swear someone needs to take it away from me cuase I'm just accident prone.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I hate badging shit in accident</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>#3: Car Recorder ZeroEdgeå¨ Dual-lens Car Camera Vehicle Traffic/Driving History/Accident Camcorder  Large Re... http://t.co/kKFaSJv6Cj</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Coincidence Or #Curse? Still #Unresolved Secrets From Past http://t.co/7VG8Df9pLE #accident</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>@Traffic_SouthE @roadpol_east Accident on A27 near Lewes is it Kingston Roundabout rather than A283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>@sakuma_en If you pretend to feel a certain way the feeling can become genuine all by accident. -Hei (Darker than Black) #manga #anime</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>For Legal and Medical Referral Service @1800_Injured Call us at: 1-800-465-87332 #accident #slipandfall #dogbite</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>There's a construction guy working on the Disney store and he has huge gauges in his ears ?? ...that is a bloody accident waiting to happen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>@RobynJilllian @WlSDOMTEETHS I feel like I'm going to do it on accident. Teesha is gonna come out??</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>On the #M42 northbound between junctions J3 and J3A there are currently delays of 10 mins due to an accident c... http://t.co/LwI3prBa31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>@DaveOshry @Soembie So if I say that I met her by accident this week- would you be super jelly Dave? :p</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ACCIDENT - HIT AND RUN - COLD at 500 BLOCK OF SE VISTA TER GRESHAM OR [Gresham Police #PG15000044357] 10:35 #pdx911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>@Calum5SOS this happened on accident but I like it http://t.co/QHmXuljSX9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Please donate and spread the word! A training accident left the pole-vaulter Kira GrÌ_nberg a paraplegic http://t.co/6MpnyCl8PK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Please like and share our new page for our Indoor Trampoline Park Aftershock opening this fall!! http://t.co/UgXhHErrxS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>@bxckylynch foi no ROH Aftershock: Las Vegas procura no pirate bay que tem</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Schoolboy ÛÒ Aftershock (Original Mix)\\nExcision &amp;amp; Skism ÛÒ SEXisM (Far Too Loud Remix)\\nFirebeatz Schella ÛÒ Dear New... http://t.co/JQLzUA6YzQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/THyzOMVWU0 | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/83jOO0xk29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>'When the aftershock happened (Nepal) we were the last int'l team still there; in a way we were 1st responders.' Chief Collins @LACo_FD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/gRPeF7yAWG | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/GGmvzT58vE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                       Text  \\\n",
       "0                                                                                                                        Just happened a terrible car crash   \n",
       "1                                                                                          Heard about #earthquake is different cities, stay safe everyone.   \n",
       "2                                                          there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all   \n",
       "3                                                                                                                  Apocalypse lighting. #Spokane #wildfires   \n",
       "4                                                                                                             Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "5                                                                                                                        We're shaking...It's an earthquake   \n",
       "6                                                                                  They'd probably still show more life than Arsenal did yesterday, eh? EH?   \n",
       "7                                                                                                                                         Hey! How are you?   \n",
       "8                                                                                                                                          What a nice hat?   \n",
       "9                                                                                                                                                 Fuck off!   \n",
       "10                                                                                                                                    No I don't like cold!   \n",
       "11                                                                                                                               NOOOOOOOOO! Don't do that!   \n",
       "12                                                                                                                                   No don't tell me that!   \n",
       "13                                                                                                                                                What if?!   \n",
       "14                                                                                                                                                 Awesome!   \n",
       "15                                 Birmingham Wholesale Market is ablaze BBC News - Fire breaks out at Birmingham's Wholesale Market http://t.co/irWqCEZWEU   \n",
       "16                                                                                                   @sunkxssedharry will you wear shorts for race ablaze ?   \n",
       "17                                            #PreviouslyOnDoyinTv: Toke MakinwaÛªs marriage crisis sets Nigerian Twitter ablaze... http://t.co/CMghxBa2XI   \n",
       "18                                       Check these out: http://t.co/rOI2NSmEJJ http://t.co/3Tj8ZjiN21 http://t.co/YDUiXEfIpE http://t.co/LxTjc87KLS #nsfw   \n",
       "19                                                        PSA: IÛªm splitting my personalities.\\n\\n?? techies follow @ablaze_co\\n?? Burners follow @ablaze   \n",
       "20                                                                                                             beware world ablaze sierra leone &amp; guap.   \n",
       "21                                                                                      Burning Man Ablaze! by Turban Diva http://t.co/hodWosAmWS via @Etsy   \n",
       "22               Not a diss song. People will take 1 thing and run with it. Smh it's an eye opener though. He is about 2 set the game ablaze @CyhiThePrynce   \n",
       "23                Rape victim dies as she sets herself ablaze: A 16-year-old girl died of burn injuries as she set herself ablazeÛ_ http://t.co/UK8hNrbOob   \n",
       "24                                                                                                             SETTING MYSELF ABLAZE http://t.co/6vMe7P5XhC   \n",
       "25              @CTVToronto the bins in front of the field by my house wer set ablaze the other day flames went rite up the hydro pole wonder if it was him   \n",
       "26                                                                         #nowplaying Alfons - Ablaze 2015 on Puls Radio #pulsradio http://t.co/aA5BJgWfDv   \n",
       "27          'Burning Rahm': Let's hope City Hall builds a giant wooden mayoral effigy 100 feet tall &amp; sets it ablaze. http://t.co/kFo2mksn6Y @John_Kass   \n",
       "28                                                                                 @PhilippaEilhart @DhuBlath hurt but her eyes ablaze with insulted anger.   \n",
       "29                                      Accident cleared in #PaTurnpike on PATP EB between PA-18 and Cranberry slow back to #traffic http://t.co/SL0Oqn0Vyr   \n",
       "30                Just got to love burning your self on a damn curling wand... I swear someone needs to take it away from me cuase I'm just accident prone.   \n",
       "31                                                                                                                          I hate badging shit in accident   \n",
       "32                  #3: Car Recorder ZeroEdgeå¨ Dual-lens Car Camera Vehicle Traffic/Driving History/Accident Camcorder  Large Re... http://t.co/kKFaSJv6Cj   \n",
       "33                                                              Coincidence Or #Curse? Still #Unresolved Secrets From Past http://t.co/7VG8Df9pLE #accident   \n",
       "34                                                      @Traffic_SouthE @roadpol_east Accident on A27 near Lewes is it Kingston Roundabout rather than A283   \n",
       "35                   @sakuma_en If you pretend to feel a certain way the feeling can become genuine all by accident. -Hei (Darker than Black) #manga #anime   \n",
       "36                                         For Legal and Medical Referral Service @1800_Injured Call us at: 1-800-465-87332 #accident #slipandfall #dogbite   \n",
       "37              There's a construction guy working on the Disney store and he has huge gauges in his ears ?? ...that is a bloody accident waiting to happen   \n",
       "38                                                      @RobynJilllian @WlSDOMTEETHS I feel like I'm going to do it on accident. Teesha is gonna come out??   \n",
       "39                 On the #M42 northbound between junctions J3 and J3A there are currently delays of 10 mins due to an accident c... http://t.co/LwI3prBa31   \n",
       "40                                                  @DaveOshry @Soembie So if I say that I met her by accident this week- would you be super jelly Dave? :p   \n",
       "41                                      ACCIDENT - HIT AND RUN - COLD at 500 BLOCK OF SE VISTA TER GRESHAM OR [Gresham Police #PG15000044357] 10:35 #pdx911   \n",
       "42                                                                                @Calum5SOS this happened on accident but I like it http://t.co/QHmXuljSX9   \n",
       "43                          Please donate and spread the word! A training accident left the pole-vaulter Kira GrÌ_nberg a paraplegic http://t.co/6MpnyCl8PK   \n",
       "44                                  Please like and share our new page for our Indoor Trampoline Park Aftershock opening this fall!! http://t.co/UgXhHErrxS   \n",
       "45                                                                               @bxckylynch foi no ROH Aftershock: Las Vegas procura no pirate bay que tem   \n",
       "46  Schoolboy ÛÒ Aftershock (Original Mix)\\nExcision &amp; Skism ÛÒ SEXisM (Far Too Loud Remix)\\nFirebeatz Schella ÛÒ Dear New... http://t.co/JQLzUA6YzQ   \n",
       "47               320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/THyzOMVWU0 | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/83jOO0xk29   \n",
       "48                  'When the aftershock happened (Nepal) we were the last int'l team still there; in a way we were 1st responders.' Chief Collins @LACo_FD   \n",
       "49               320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/gRPeF7yAWG | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/GGmvzT58vE   \n",
       "\n",
       "    Pred Label  Confidence  \n",
       "0          1.0       97.32  \n",
       "1          1.0       99.33  \n",
       "2          1.0       98.19  \n",
       "3          1.0       91.52  \n",
       "4          1.0       92.66  \n",
       "5          1.0       93.77  \n",
       "6          0.0       96.72  \n",
       "7          0.0       97.66  \n",
       "8          0.0       98.27  \n",
       "9          0.0       97.64  \n",
       "10         0.0       94.46  \n",
       "11         0.0       93.22  \n",
       "12         0.0       95.79  \n",
       "13         0.0       90.97  \n",
       "14         0.0       98.83  \n",
       "15         1.0       88.36  \n",
       "16         0.0       81.10  \n",
       "17         0.0       82.38  \n",
       "18         0.0       98.32  \n",
       "19         1.0       70.28  \n",
       "20         0.0       94.96  \n",
       "21         0.0       76.40  \n",
       "22         0.0       92.41  \n",
       "23         1.0       98.03  \n",
       "24         0.0       87.58  \n",
       "25         1.0       77.94  \n",
       "26         1.0       54.05  \n",
       "27         1.0       96.71  \n",
       "28         0.0       86.64  \n",
       "29         1.0       98.66  \n",
       "30         0.0       96.08  \n",
       "31         0.0       86.05  \n",
       "32         0.0       85.43  \n",
       "33         0.0       92.77  \n",
       "34         1.0       95.29  \n",
       "35         0.0       98.10  \n",
       "36         0.0       94.32  \n",
       "37         0.0       56.32  \n",
       "38         0.0       98.28  \n",
       "39         1.0       98.99  \n",
       "40         0.0       98.89  \n",
       "41         1.0       98.47  \n",
       "42         1.0       65.38  \n",
       "43         0.0       82.24  \n",
       "44         0.0       98.70  \n",
       "45         0.0       95.71  \n",
       "46         0.0       98.50  \n",
       "47         0.0       98.80  \n",
       "48         1.0       92.44  \n",
       "49         0.0       99.10  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame({'Text': test_sentences.tolist(), 'Pred Label': model_6_pred, 'Confidence': confidence_test})\n",
    "test_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ab57d-a6a1-4887-9064-e195cdf7aca7",
   "metadata": {},
   "source": [
    "## Lets think about the speed/score tradeoff\n",
    "\n",
    "Our best performing model is more accurate by about 2% but at what cost. Lets say we really work for twitter and we are seeing 1 million tweets per day. What if our deep model can only look at half those tweets because of the speed in which it predicts but the baseline model can look at all of them. Is that worth the tradeoff of 2% accuracy?\n",
    "\n",
    "Lets run some speed tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "e4515cdf-d552-4b36-8890-3d78942057f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def pred_timer(model, samples):\n",
    "    start_time = time.perf_counter() \n",
    "    model.predict(samples)\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time #calculate how long predictions took to make\n",
    "    time_per_pred = total_time/len(samples)\n",
    "    return total_time, time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "f3b863da-180f-488d-8839-ea19a470feef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1453703340375796, 0.00019077471658475012)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate our best models time per prediction\n",
    "model_6_total_time, model_6_time_per_pred = pred_timer(model_6, samples = val_sentences)\n",
    "model_6_total_time, model_6_time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "a17aa731-d6aa-4db8-940d-d107f05d91ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.025670750066637993, 3.368864838141469e-05)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate baseline models time per pred\n",
    "baseline_total_time, baseline_time_per_pred = pred_timer(model_0, val_sentences)\n",
    "baseline_total_time, baseline_time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "427310ab-3f8e-4d01-b4a7-8bd48a4189f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.66287832105477, 5.66287832105477)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time_diff = model_6_total_time/baseline_total_time\n",
    "per_pred_diff = model_6_time_per_pred/baseline_time_per_pred\n",
    "\n",
    "total_time_diff, per_pred_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3104d31-e5d7-4065-a7bc-d3e0c569ef91",
   "metadata": {},
   "source": [
    "The baseline model is about 6x faster than the deep model with only 2% less accuracy. It would be up to the client which model they would prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaf5a69-25c2-40b5-b3fc-e5411da5530a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
